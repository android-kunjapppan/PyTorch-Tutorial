{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_AutoGrad.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPvdIS7BZtxuH81avjPVceW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/android-kunjapppan/PyTorch-Tutorial/blob/master/PyTorch_AutoGrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL7g348je7sg",
        "colab_type": "text"
      },
      "source": [
        "* To calculate gradients in pytorch\n",
        "* Automatically creates and stores a function to get the graidents\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hImAPl4des6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "22f99681-4f37-424f-9a80-310f0840e5ea"
      },
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(3,requires_grad=True)\n",
        "# if requires_grad = False then backward() will give an error\n",
        "print(x)\n",
        "\n",
        "y = x+1\n",
        "print(y) # this will print as grad_fn =<AddBackward0>\n",
        "\n",
        "z = y*y*2\n",
        "print(z) # this will print as grad_fn =<MulBackward0>\n",
        "\n",
        "z=z.mean()\n",
        "print(z) # this will print as grad_fn =<MeanBackward0>\n",
        "\n",
        "\n",
        "# TO calculate the gradients\n",
        "z.backward()  # dz/dx\n",
        "\n",
        "# grad can be implicitly created only for scalar outputs\n",
        "# If it is not a scalar, then a vector must be passed as an argument in the backward()\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.1112, -0.3664, -0.4563], requires_grad=True)\n",
            "tensor([1.1112, 0.6336, 0.5437], grad_fn=<AddBackward0>)\n",
            "tensor([2.4694, 0.8028, 0.5913], grad_fn=<MulBackward0>)\n",
            "tensor(1.2878, grad_fn=<MeanBackward0>)\n",
            "tensor([1.4816, 0.8447, 0.7250])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p_9uJZ0helg",
        "colab_type": "text"
      },
      "source": [
        "# To prevent from tracking the gradients\n",
        "1. x.requires_grad_(False)\n",
        "2. x.detach()\n",
        "3. with torch.no_grad():\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ot0G_OUe_07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "1c1b172f-144d-48e2-9bb9-8f06db2033fd"
      },
      "source": [
        "x1 = torch.randn(3,requires_grad=True)\n",
        "print(x1)\n",
        "\n",
        "x1.requires_grad_(False)\n",
        "print(x1)\n",
        "\n",
        "y = x.detach() # this will create a new tensor with same values but doesnt require gradients\n",
        "print(y)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y1 = x1+2\n",
        "  print(y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.0032, -1.0881, -0.4148], requires_grad=True)\n",
            "tensor([ 0.0032, -1.0881, -0.4148])\n",
            "tensor([ 0.1112, -0.3664, -0.4563])\n",
            "tensor([ 0.1112, -0.3664, -0.4563])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neqD2KfOjCye",
        "colab_type": "text"
      },
      "source": [
        "# setting Gradients after each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sknR7B1Fi8EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "baecffaa-03cc-49a4-b48b-07b641194528"
      },
      "source": [
        "weights = torch.ones(4,requires_grad = True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights *3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "\n",
        "  # we must empty the gradients before going into next iteration\n",
        "  weights.grad.zero_()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M3u3dTikKWp",
        "colab_type": "text"
      },
      "source": [
        "# INbuilt optimizer functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx2nsBSijsM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optim = torch.optim.SGD(weights,lr = 0.01)\n",
        "optim.step()\n",
        "optim.zero_grad()"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}